### Sorted by Year

[Otter: A multi-modal model with in-context instruction tuning](https://arxiv.org/abs/2305.03726)
  - **Authors:** B Li, Y Zhang, L Chen, J Wang, J Yang…
  - **Year:** 2023
  - **Cites:** 148
  - **Abstract:** … In this work, we propose Otter, a multi-modal in-context learning foundation model with instruction tuning. Through partial finetuning on MIMIC-IT dataset, we observe that Otter can …

[Mimic-it: Multi-modal in-context instruction tuning](https://arxiv.org/abs/2306.05425)
  - **Authors:** B Li, Y Zhang, L Chen, J Wang, F Pu, J Yang…
  - **Year:** 2023
  - **Cites:** 43
  - **Abstract:** … distinguishes itself by incorporating a multi-modal in-context format into instruction tuning. … into an in-context instruction tuning format, based on the previously established guidelines. …

[Few-shot Fine-tuning vs. In-context Learning: A Fair Comparison and Evaluation](https://arxiv.org/abs/2305.16938)
  - **Authors:** M Mosbach, T Pimentel, S Ravfogel, D Klakow…
  - **Year:** 2023
  - **Cites:** 11
  - **Abstract:** … , in-context learning has gained popularity over fine-tuning … models is an inherent property of finetuning or a limitation of the … of few-shot fine-tuning and in-context learning to challenge …

[Symbol tuning improves in-context learning in language models](https://arxiv.org/abs/2305.08298)
  - **Authors:** J Wei, L Hou, A Lampinen, X Chen, D Huang…
  - **Year:** 2023
  - **Cites:** 10
  - **Abstract:** … To make the model better at in-context learning, we propose symbol tuning, in which the model is finetuned on exemplars where the instructions are removed and natural language …

[How does in-context learning help prompt tuning?](https://arxiv.org/abs/2302.11521)
  - **Authors:** S Sun, Y Liu, D Iter, C Zhu, M Iyyer
  - **Year:** 2023
  - **Cites:** 7
  - **Abstract:** … to an otherwise frozen model, and in-context learning (ICL), in … and how in-context examples improve prompt tuning by … in fact requires the in-context demonstration to be semantically …

[GersteinLab at MEDIQA-Chat 2023: Clinical Note Summarization from Doctor-Patient Conversations through Fine-tuning and In-context Learning](https://arxiv.org/abs/2305.05001)
  - **Authors:** X Tang, A Tran, J Tan, M Gerstein
  - **Year:** 2023
  - **Cites:** 5
  - **Abstract:** … fine-tuning of OpenAI’s Davinci model. For subtask B, we perform fine-tuning as well as few-shot in-context … used a RoBERTa model and fine-tuning of OpenAI’s Davinci model. We fine-…

[ScoNe: Benchmarking Negation Reasoning in Language Models With Fine-Tuning and In-Context Learning](https://arxiv.org/abs/2305.19426)
  - **Authors:** JS She, C Potts, SR Bowman, A Geiger
  - **Year:** 2023
  - **Cites:** 3
  - **Abstract:** … -tuning as well as a wide range of incontext learning strategies. For fine-tuning approaches, we find that RoBERTa and DeBERTa models both solve ScoNe-NLI. For in-context learning, …

[Iterative Forward Tuning Boosts In-context Learning in Language Models](https://arxiv.org/abs/2305.13016)
  - **Authors:** J Yang, B Hui, M Yang, B Li, F Huang, Y Li
  - **Year:** 2023
  - **Cites:** 2
  - **Abstract:** Large language models (LLMs) have exhibited an emergent in-context learning (ICL) ability. However, the ICL models that can solve ordinary cases are hardly extended to solve more …

[Exploring the Relationship between In-Context Learning and Instruction Tuning](https://arxiv.org/abs/2311.10367)
  - **Authors:** H Duan, Y Tang, Y Yang, A Abbasi, KY Tam
  - **Year:** 2023
  - **Cites:** 1
  - **Abstract:** … In this work, we explore the connection between in-context learning (ICL) and instruction tuning (IT). Through carefully designed experiments, we provide strong evidences suggesting …

[Lightweight In-Context Tuning for Multimodal Unified Models](https://arxiv.org/abs/2310.05109)
  - **Authors:** Y Chen, S Zhang, B Han, J Jia
  - **Year:** 2023
  - **Cites:** 1
  - **Abstract:** … • We propose M2IXT, an in-context tuning module explicitly designed to enable multimodal unified models to conduct in-context learning effectively. M2IXT can deal with multimodal …

[COSMIC: Data Efficient Instruction-tuning For Speech In-Context Learning](https://arxiv.org/abs/2311.02248)
  - **Authors:** J Pan, J Wu, Y Gaur, S Sivasankaran, Z Chen…
  - **Year:** 2023
  - **Cites:** 1
  - **Abstract:** … After the proposed speech instruction-tuning, COSMIC develops few-shot in-context learning ability that makes it possible to achieve unseen tasks. We enable speech in-context …

[In-Context Alignment: Chat with Vanilla Language Models Before Fine-Tuning](https://arxiv.org/abs/2308.04275)
  - **Authors:** X Han
  - **Year:** 2023
  - **Cites:** 1
  - **Abstract:** … in-context learning. We consider a vanilla pretrained language model Llama-2 before any fine-tuning … Compared to direct prompting, the in-context alignment without changing model …

[MMICT: Boosting Multi-Modal Fine-Tuning with In-Context Examples](https://arxiv.org/abs/2312.06363)
  - **Authors:** T Chen, E Zhang, Y Gao, K Li, X Sun, Y Zhang…
  - **Year:** 2023
  - **Cites:** 0
  - **Abstract:** … to further enhance the finetuning performance on downstream … InContext Tuning (MMICT), a novel multi-modal fine-tuning paradigm that harnesses ICL to improve multi-modal finetuning…

[Parameterizing Context: Unleashing the Power of Parameter-Efficient Fine-Tuning and In-Context Tuning for Continual Table Semantic Parsing](https://arxiv.org/abs/2310.04801)
  - **Authors:** Y Chen, S Zhang, G Qi, X Guo
  - **Year:** 2023
  - **Cites:** 0
  - **Abstract:** … integrating parameter-efficient fine-tuning (PEFT) and in-context tuning (ICT) for training a … freezing the pre-trained model backbone and fine-tuning small-scale prompts. Building on this, …

[Widespread but limited in context: Absolute tuning judgments are disrupted by relative pitch cues](https://journals.sagepub.com/doi/abs/10.1177/20592043231208626)
  - **Authors:** SCV Hedger, NR Bongiovanni
  - **Year:** 2023
  - **Cites:** 0
  - **Abstract:** … , the tuning of each … tuning option to the default ( + 0 cents; A440 tuning), whereas the out-of-tune sounds were exported by setting the master tuning option to + 50 cents (∼A453 tuning). …

[Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning](https://arxiv.org/abs/2310.11397)
  - **Authors:** R Wen, T Wang, M Backes, Y Zhang…
  - **Year:** 2023
  - **Cites:** 0
  - **Abstract:** … [16] that compare the information leakages (using membership inference) in fine-tuned models and in-context learning, our approach provides a more comprehensive comparison that …

[ScoNe: Benchmarking Negation Reasoning in Language Models With Fine-Tuning and In-Context Learning](https://ui.adsabs.harvard.edu/abs/2023arXiv230519426S/abstract)
  - **Authors:** J Selena She, C Potts, SR Bowman…
  - **Year:** 2023
  - **Cites:** 0
  - **Abstract:** … We use ScoNe-NLI to assess fine-tuning and in-context learning strategies. We find that RoBERTa and DeBERTa models solve ScoNe-NLI after many shot fine-tuning. For in-context …

[Cross-Task Generalization via In-Context Tuning](https://etd.adm.unipi.it/t/etd-02012023-182023/)
  - **Authors:** D ANNISA
  - **Year:** 2023
  - **Cites:** 0
  - **Abstract:** … in-context tuning … This method is called in-context tuning in the literature. We fine-tune T5 base and large using this technique and evaluate it using CrossFit, a repository of few-shot …

[Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning](https://proceedings.neurips.cc/paper_files/paper/2022/hash/0cde695b83bd186c1fd456302888454c-Abstract-Conference.html)
  - **Authors:** H Liu, D Tam, M Muqeeth, J Mohta…
  - **Year:** 2022
  - **Cites:** 243
  - **Abstract:** … to process all of the in-context labeled examples. Specifically, … required for storing the in-context examples for a given task. … well even if the in-context example labels are swapped (ie …

[Automated scoring for reading comprehension via in-context bert tuning](https://link.springer.com/chapter/10.1007/978-3-031-11644-5_69)
  - **Authors:** N Fernandez, A Ghosh, N Liu, Z Wang…
  - **Year:** 2022
  - **Cites:** 16
  - **Abstract:** … We compare our approach, meta-trained BERT with in-context tuning, with existing … BERT in-context adds in-context examples. BERT multi-task uses multi-task learning to fine-…

[Preserving In-Context Learning ability in Large Language Model Fine-tuning](https://arxiv.org/abs/2211.00635)
  - **Authors:** Y Wang, S Si, D Li, M Lukasik, F Yu, CJ Hsieh…
  - **Year:** 2022
  - **Cites:** 7
  - **Abstract:** … ’s in-context capabilities during fine-tuning, … in-context prompts in a closed book setting. Figure 1 shows that when the accuracy on RTE dataset increases with fine-tuning, the in-context …

[Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning (2022)]()
  - **Authors:** H Liu, D Tam, M Muqeeth, J Mohta, T Huang, M Bansal…
  - **Year:** 2022
  - **Cites:** 3
  - **Abstract:** 

[… Li, Michal Lukasik, Felix Yu, Cho-Jui Hsieh, Inderjit S Dhillon, and Sanjiv Kumar. Preserving in-context learning ability in large language model fine-tuning]()
  - **Authors:** Y Wang
  - **Year:** 2022
  - **Cites:** 2
  - **Abstract:** 

[DiSTRICT: Dialogue State Tracking with Retriever Driven In-Context Tuning](https://arxiv.org/abs/2212.02851)
  - **Authors:** P Venkateswaran, E Duesterwald…
  - **Year:** 2022
  - **Cites:** 1
  - **Abstract:** … In this work, we propose DiSTRICT, a generalizable in-context tuning approach for DST that retrieves highly relevant training examples for a given dialogue to fine-tune the model …

[Robust coding of eye position in posterior parietal cortex despite context-dependent tuning](https://www.jneurosci.org/content/42/20/4116.abstract)
  - **Authors:** JR McFadyen, B Heider, AN Karkhanis…
  - **Year:** 2022
  - **Cites:** 0
  - **Abstract:** … This tuning, however, changed within trials as the context changed, reversing spatial … tuning suggests that the population code for eye position could be corrupted by changes in context. …

[Meta-learning via language model in-context tuning](https://arxiv.org/abs/2110.07814)
  - **Authors:** Y Chen, R Zhong, S Zha, G Karypis, H He
  - **Year:** 2021
  - **Cites:** 65
  - **Abstract:** … in large language models, we propose in-context tuning (ICT), which recasts task … in-context examples, and the target input to predict; to metatrain the model to learn from in-context ex…

[Role of the NF-κB system in context-specific tuning of the inflammatory gene response](https://www.sciencedirect.com/science/article/pii/S0952791520300819)
  - **Authors:** M Chawla, P Roy, S Basak
  - **Year:** 2021
  - **Cites:** 29
  - **Abstract:** … Here, we discuss how the NF-κB system orchestrates context-specific tuning of inflammatory gene response. We have focused on NF-κB-mediated direct gene activation mechanisms; …

[Uppsala nlp at semeval-2021 task 2: Multilingual language models for fine-tuning and feature extraction in word-in-context disambiguation](https://arxiv.org/abs/2104.03767)
  - **Authors:** H You, X Zhu, S Stymne
  - **Year:** 2021
  - **Cites:** 2
  - **Abstract:** … We have investigated the use of three large language models for multilingual and cross-lingual word-in-context disambiguation. We found that fine-tuning the language models is …

[Spectral tuning of adaptation supports coding of sensory context in auditory cortex](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007430)
  - **Authors:** M Lopez Espejo, ZP Schwartz…
  - **Year:** 2019
  - **Cites:** 20
  - **Abstract:** … When used to study auditory cortex, however, LN models typically measure tuning properties only … These findings suggest that an adaptation mechanism plays a central role in context-…

[… interactions in context: the finding that immobilized ions can alter the strength of hydrophobic interactions between molecules suggests a strategy for tuning …](https://go.gale.com/ps/i.do?id=GALE%7CA398253318&sid=googleScholar&v=2.1&it=r&linkaccess=abs&issn=00280836&p=HRCA&sw=w)
  - **Authors:** S Garde
  - **Year:** 2015
  - **Cites:** 0
  - **Abstract:** Oil and water do not mix. At the molecular level, this' de-mixing'tendency, known as the hydrophobic interaction, is thought to drive many self-assembly processes, such as protein folding…

[Fine-tuning model transformation: Change propagation in context of consistency, completeness, and human guidance](https://link.springer.com/chapter/10.1007/978-3-642-21732-6_1)
  - **Authors:** A Egyed, A Demuth, A Ghabi, R Lopez-Herrejon…
  - **Year:** 2011
  - **Cites:** 11
  - **Abstract:** … should be to reason about the logical implications of changes in context of diverse models. … This issue, however, becomes trickier in context of changes that carry across multiple models. …

[Roles of pre-training and fine-tuning in context-dependent DBN-HMMs for real-world speech recognition](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/dbn4asr-nips2010.pdf)
  - **Authors:** D Yu, L Deng, G Dahl
  - **Year:** 2010
  - **Cites:** 259
  - **Abstract:** … -- pre-training and fine tuning -- in the recognition performance … point in the space where fine-tuning can be effective and thus is … training data, the fine-tuning phase of DBN training can …

[Code Tuning in Context.](https://elibrary.ru/item.asp?id=3581272)
  - **Authors:** J Bentley
  - **Year:** 1999
  - **Cites:** 2
  - **Abstract:** Code Tuning in Context. … Studies code tuning loop in several contexts. Changes in code tuning; Application of four tuneups to four related algorithms; Measurement of results on …

[Effect of melodic parameters on ability to make fine-tuning responses in context](https://journals.sagepub.com/doi/abs/10.2307/3344768)
  - **Authors:** WR Swaffield
  - **Year:** 1974
  - **Cites:** 45
  - **Abstract:** … The effect upon fine tuning of four contextual melodic factors: timbre, intensity… tuning, and that there is a strong tendency for subjects to be influenced by the initial frequency of the tuning …

[The effect of certain melodic parameters upon the ability to make fine-tuning responses in context]()
  - **Authors:** WR Swaffield
  - **Year:** 1973
  - **Cites:** 0
  - **Abstract:** 

[TO MAKE FINE-TUNING RESPONSES IN CONTEXT](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=e05077fb982f3c34150756b47ef0877f4c563787)
  - **Authors:** WR Swaffield
  - **Year:** 
  - **Cites:** 0
  - **Abstract:** … The effect upon fine tuning of four contextual melodic factors: timbre, intensity… tuning, and that there is a strong tendency for subjects to be influenced by the initial frequency of the tuning …

[Few-shot Fine-tuning vs. In-context Learning: A Fair Comparison and Evaluation](https://www.lsv.uni-saarland.de/wp-content/uploads/2023/07/Few-shot-Fine-tuning-vs.-In-context-Learning.pdf)
  - **Authors:** MMTPS Ravfogel, DKY Elazar
  - **Year:** 
  - **Cites:** 0
  - **Abstract:** … , in-context learning has gained popularity over fine-tuning … models is an inherent property of finetuning or a limitation of the … of few-shot fine-tuning and in-context learning to challenge …

